{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archita924/CSA522_ML/blob/master/examples/vision/ipynb/mnist_convnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqEch3mIhVhL"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Every neural network is made up of layers â€” each performs a specific job:\n",
        "â€¢\tConv2D: Extracts image features\n",
        "â€¢\tMaxPooling2D: Reduces size\n",
        "â€¢\tFlatten: Converts 2D to 1D\n",
        "â€¢\tDense: Makes final decisions (classification)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ibxO_FgqjIP0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "weP1dF4IhVhM"
      },
      "outputs": [],
      "source": [
        "import numpy as np   #  numerical computation eg.Handle image pixels ,labels, and features\n",
        "import keras        # build train,test neural networks\n",
        "from keras import layers  # building blocks of neural net i.e. Conv2D ,MaxPooling,Flatten,Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0aLJrv2hVhN"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\" why num_classes is 10?\"\n",
        "ğŸ§  Explain:\n",
        "Because the MNIST dataset has 10 classes â€” digits 0 to 9.\n",
        "Each image is a handwritten number.\n",
        "\"And what about (28, 28, 1) â€” why do we have that extra 1?\"\n",
        "ğŸ’¡ Answer:\n",
        "â€¢\t28 x 28 = image size (pixels).\n",
        "â€¢\t1 = grayscale channel (since MNIST images are black & white).\n",
        "If it were a color image, it would be (28, 28, 3) for RGB.\n"
      ],
      "metadata": {
        "id": "Ub0m7Oa4lC9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)"
      ],
      "metadata": {
        "id": "iSAdQeGQkw7n"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras automatically downloads and loads the MNIST dataset for you!\n",
        "It gives you:\n",
        "â€¢\tx_train: training images\n",
        "â€¢\ty_train: correct digit labels for training\n",
        "â€¢\tx_test and y_test: for testing the model later\n"
      ],
      "metadata": {
        "id": "25MXlzzglvdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "Slkj3SyRlt_S"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mCXQSNX5mQss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Why do we divide by 255?\"\n",
        "\n",
        "ğŸ§  Explain:\n",
        "Each pixelâ€™s intensity ranges from 0 to 255.\n",
        "Dividing by 255 converts it to 0â€“1, making it easier for the neural network to learn.\n",
        "(It trains faster and avoids large number errors.)\n",
        "\n",
        "\"Think of it like normalizing marks from 0â€“100 to 0â€“1 â€” easier to compare!\"\n"
      ],
      "metadata": {
        "id": "AhLiNIXNmTr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") /255"
      ],
      "metadata": {
        "id": "-dm180WAmPO0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding the Channel Dimension **\n",
        "\n",
        "\n",
        "\"Why are we adding one more dimension?\"\n",
        "ğŸ§  Explain:\n",
        "\n",
        "Originally, x_train shape is (60000, 28, 28) â†’ just height and width.\n",
        "But CNNs expect input like (height, width, channels).\n",
        "\n",
        "So we add that 1 channel for grayscale using np.expand_dims.\n"
      ],
      "metadata": {
        "id": "Rzsg0gqsmwX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)"
      ],
      "metadata": {
        "id": "ixbRBKWSmuvM"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking Data Shapes**"
      ],
      "metadata": {
        "id": "UBJ1pOWSn_YZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdlj2BAnn9kT",
        "outputId": "f6b4b0dd-dead-4e6f-e96a-429031b78ad0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting Labels to One-Hot Encoding**\n",
        "\n",
        "Neural networks work better when each class is represented as a vector, not just a number.\n",
        "Example:\n",
        "Digit 3 â†’ [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "Digit 7 â†’ [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
        "\n",
        "\n",
        "This is called One-Hot Encoding ğŸ”¥\n",
        "\n",
        "It helps the network treat all classes equally instead of â€œcloserâ€ numbers (like 8 being near 9).\n"
      ],
      "metadata": {
        "id": "ruHPzetDoVVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "jQfCwHBuoMYc"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moHC0LGChVhO"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1:\n",
        "\n",
        "\n",
        "model = keras.Sequential([...])\n",
        "\n",
        "Thatâ€™s exactly what Sequential means!\n",
        "\n",
        "Each layer is stacked in order, and the output of one becomes the input of the next.\n",
        "\n",
        "Step 2:\n",
        "\n",
        "\n",
        "keras.Input(shape=input_shape)\n",
        "ğŸ§  Explain:\n",
        "\n",
        "This defines the shape of each input image â€” (28, 28, 1) = height, width, and grayscale channel.\n",
        "\n",
        "ğŸ’¬ Analogy:\n",
        "â€œThis is like telling the model â€”\n",
        "\n",
        "â€˜Hey, every image youâ€™ll see is 28x28 pixels and black & white!â€™â€\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ğŸ¨ Step 3: First Convolutional Layer\n",
        "layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\")\n",
        "\n",
        "ğŸ§  Explain :\n",
        "\n",
        "\"Think of this layer as 32 small scanners (filters) sliding over the image â€” each trying to detect different patterns like edges, curves, or corners.\"\n",
        "\n",
        "â€¢\t32 â†’ number of filters (features the model will learn)\n",
        "\n",
        "â€¢\t(3, 3) â†’ size of each filter (like a small 3x3 window)\n",
        "\n",
        "â€¢\tReLU â†’ removes negative values â†’ keeps only useful signals\n",
        "\n",
        "ğŸ’¬ Analogy:\n",
        "â€œItâ€™s like shining 32 tiny flashlights on different parts of the image to detect unique features.â€\n",
        "\n",
        "\n",
        "ğŸŒ€ Step 4: First Pooling Layer\n",
        "layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "\n",
        "ğŸ§  Explain:\n",
        "This layer shrinks the image while keeping the important parts.\n",
        "\n",
        "â€¢\tTakes a 2Ã—2 patch â†’ picks the maximum value\n",
        "\n",
        "â€¢\tReduces computation and helps the model focus on key patterns\n",
        "\n",
        "\n",
        "ğŸ’¬ Analogy:\n",
        "â€œThink of it like zooming out of a photo â€” you lose some detail, but you still recognize whatâ€™s important.â€\n",
        "\n",
        "ğŸ¨ Step 5: Second Convolutional Layer\n",
        "layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\")\n",
        "\n",
        "ğŸ§  Explain:\n",
        "Now the model learns more complex patterns using 64 filters.\n",
        "\n",
        "After the first layer learned simple edges, this one can detect shapes, loops, or digit structures.\n",
        "\n",
        "ğŸ’¬ Analogy:\n",
        "â€œThe model is now learning to recognize numbers, not just lines â€” like a student going from alphabets to words.â€\n",
        "\n",
        "\n",
        "\n",
        "ğŸŒ€ Step 6: Second Pooling Layer\n",
        "layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "\n",
        "ğŸ§  Explain:\n",
        "Again reduces the size, keeping only essential patterns.\n",
        "\n",
        "\n",
        "Now the image is small, but contains deep, meaningful information.\n",
        "\n",
        "\n",
        "ğŸ§¾ Step 7: Flatten Layer\n",
        "layers.Flatten()\n",
        "\n",
        "\n",
        "ğŸ§  Explain:\n",
        "This takes the 2D feature maps and flattens them into a 1D vector.\n",
        "\n",
        "\n",
        "\n",
        "ğŸ’§ Step 8: Dropout Layer\n",
        "layers.Dropout(0.5)\n",
        "\n",
        "\n",
        "ğŸ§  Explain:\n",
        "This randomly â€œturns offâ€ 50% of neurons during training to prevent overfitting.\n",
        "\n",
        "\n",
        "ğŸ’¬ Ask students:\n",
        "â€œWhy would we want to drop neurons?â€\n",
        "\n",
        "\n",
        "âœ… To make sure the model doesnâ€™t memorize the training data and can generalize better\n",
        "\n",
        "\n",
        "\n",
        "ğŸ§  Step 9: Output Layer\n",
        "layers.Dense(num_classes, activation=\"softmax\")\n",
        "\n",
        "\n",
        "ğŸ§  Explain:\n",
        "â€¢\tDense = fully connected layer (every neuron connects to every output)\n",
        "\n",
        "\n",
        "â€¢\tnum_classes = 10 (digits 0â€“9)\n",
        "\n",
        "â€¢\tSoftmax â†’ converts outputs into probabilities (like: 80% chance of being â€œ3â€, 15% chance of being â€œ5â€, etc.)\n"
      ],
      "metadata": {
        "id": "jakQmVkXo07F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "OnTwix31hVhO",
        "outputId": "8fa9d17b-e15b-4d1d-aa3c-4326ad7862dd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚           \u001b[38;5;34m320\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚        \u001b[38;5;34m18,496\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_9 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚        \u001b[38;5;34m16,010\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,010</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,826\u001b[0m (136.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,826</span> (136.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,826\u001b[0m (136.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,826</span> (136.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wef5ipnDAgNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model**\n"
      ],
      "metadata": {
        "id": "ZCoX8MJnAjfn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abTsc1yshVhP"
      },
      "source": [
        "\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 15\n",
        "\n",
        "\n",
        "â€¢\tBatch size (128):\n",
        "The model doesnâ€™t look at all 60,000 images at once (thatâ€™s too heavy!).\n",
        "\n",
        "Instead, it studies 128 images at a time, learns from them, updates weights, and repeats.\n",
        "\n",
        "ğŸ’¬ Analogy:\n",
        "â€œThink of it like studying in small groups instead of the entire class at once.â€\n",
        "\n",
        "\n",
        "â€¢\tEpochs (15):\n",
        "One epoch = the model has seen all training images once.\n",
        "\n",
        "\n",
        "So, with 15 epochs, it studies the dataset 15 times, improving its understanding each round.\n",
        "\n",
        "\n",
        "ğŸ’¬ Analogy:\n",
        "â€œLike rereading your notes 15 times â€” you understand better with every pass!â€\n",
        "\n",
        "â€œWhy do we need to compile before training?â€\n",
        "\n",
        "ğŸ§  Explain:\n",
        "This tells the model how to learn â€” like giving instructions before starting a class.\n",
        "\n",
        "â€¢\tloss=\"categorical_crossentropy\"\n",
        "\n",
        "â†’ This measures how wrong the modelâ€™s predictions are (for multi-class classification).\n",
        "The model tries to minimize this loss.\n",
        "\n",
        "\n",
        "\n",
        "â€¢\toptimizer=\"adam\"\n",
        "\n",
        "â†’ Adam is a smart algorithm that updates weights automatically and efficiently.\n",
        "\n",
        "It helps the model converge (learn fast and accurately).\n",
        "\n",
        "ğŸ’¬ Analogy:\n",
        "â€œAdam is like an intelligent coach â€” it adjusts your learning rate dynamically.â€\n",
        "\n",
        "â€¢\tmetrics=[\"accuracy\"]\n",
        "â†’ We track accuracy during training â€” how many predictions are correct.\n",
        "\n",
        "\n",
        "\n",
        "â€œWhat do you think happens when we call fit()?â€\n",
        "\n",
        "\n",
        "ğŸ§  Explain:\n",
        "This is where the real training happens.\n",
        "\n",
        "\n",
        "â€¢\tThe model takes input images, predicts outputs, compares them with the correct labels (y_train),\n",
        "and updates itself to reduce the loss â€” over and over again.\n",
        "\n",
        "\n",
        "â€¢\tvalidation_split=0.1 means:\n",
        "10% of training data is kept aside for validation (to check how well the model generalizes while learning).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 15\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwFrikIh9hFY",
        "outputId": "acee479a-af4c-4668-d0d1-3bc6d034663c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.7608 - loss: 0.7656 - val_accuracy: 0.9750 - val_loss: 0.0881\n",
            "Epoch 2/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9622 - loss: 0.1236 - val_accuracy: 0.9843 - val_loss: 0.0567\n",
            "Epoch 3/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9718 - loss: 0.0914 - val_accuracy: 0.9870 - val_loss: 0.0449\n",
            "Epoch 4/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9782 - loss: 0.0699 - val_accuracy: 0.9875 - val_loss: 0.0416\n",
            "Epoch 5/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9790 - loss: 0.0664 - val_accuracy: 0.9880 - val_loss: 0.0423\n",
            "Epoch 6/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9829 - loss: 0.0545 - val_accuracy: 0.9897 - val_loss: 0.0365\n",
            "Epoch 7/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9843 - loss: 0.0503 - val_accuracy: 0.9907 - val_loss: 0.0330\n",
            "Epoch 8/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9847 - loss: 0.0489 - val_accuracy: 0.9912 - val_loss: 0.0319\n",
            "Epoch 9/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9862 - loss: 0.0439 - val_accuracy: 0.9908 - val_loss: 0.0329\n",
            "Epoch 10/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9860 - loss: 0.0436 - val_accuracy: 0.9908 - val_loss: 0.0318\n",
            "Epoch 11/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9878 - loss: 0.0381 - val_accuracy: 0.9923 - val_loss: 0.0311\n",
            "Epoch 12/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9885 - loss: 0.0368 - val_accuracy: 0.9917 - val_loss: 0.0288\n",
            "Epoch 13/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9893 - loss: 0.0357 - val_accuracy: 0.9920 - val_loss: 0.0287\n",
            "Epoch 14/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9889 - loss: 0.0340 - val_accuracy: 0.9917 - val_loss: 0.0297\n",
            "Epoch 15/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9890 - loss: 0.0336 - val_accuracy: 0.9928 - val_loss: 0.0277\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79fff05df680>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UodAXCXhhVhQ"
      },
      "source": [
        "## Evaluate the trained model\n",
        "\n",
        "*  â€œSo, we trained the model on training data â€” but how do we know if it really understands digits and isnâ€™t just memorizing them?â€\n",
        "\n",
        "\n",
        "ğŸ§  Explain:\n",
        "Thatâ€™s exactly what model.evaluate() does.\n",
        "It checks how well the model performs on test data â€” data it has never seen before.\n",
        "\n",
        "\n",
        "â€¢\tscore[0] â†’ Test loss\n",
        "Measures how much error the model still makes on unseen data.\n",
        "\n",
        "\n",
        "â¤ Lower = better\n",
        "â€¢\tscore[1] â†’ Test accuracy\n",
        "Tells what percentage of images the model classified correctly.\n",
        "\n",
        "\n",
        "â¤ Closer to 1 (or 100%) = better\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOQ83KyAhVhQ",
        "outputId": "5e473076-6861-4822-9836-61d51daa3b46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.023872407153248787\n",
            "Test accuracy: 0.9922999739646912\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mnist_convnet",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}